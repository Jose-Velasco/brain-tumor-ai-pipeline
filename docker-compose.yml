services:
  dev:
      build:
        dockerfile: Dockerfile
      container_name: dev
      environment:
        - MLFLOW_TRACKING_URI=http://mlflow:5000
        - CUDA_VISIBLE_DEVICES=0
        - RAY_memory_monitor_refresh_ms=0  # avoids false OOM warnings
      shm_size: "8g"    # helpful for PyTorch/ORT buffers
      ### doesnt work in MacOS, CPU only?
      # deploy:
      #   resources:
      #     reservations:
      #       devices:
      #         - driver: nvidia
      #           count: all
      #           capabilities: [gpu]

      restart: unless-stopped
      volumes:
        - .:/app
      command: sleep infinity
  
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama   # cache models here
    # GPU acceleration (NVIDIA)
    runtime: nvidia # for older Docker setups
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

volumes:
  ollama-models: