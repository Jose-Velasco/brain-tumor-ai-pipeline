services:
  dev:
      build:
        dockerfile: Dockerfile
      container_name: dev
      environment:
        - MLFLOW_TRACKING_URI=http://mlflow:5000
        - CUDA_VISIBLE_DEVICES=0
        - RAY_memory_monitor_refresh_ms=0  # avoids false OOM warnings
        - MODEL_ROOT_DIR=/app/app/models/artifacts
      shm_size: "8g"    # helpful for PyTorch/ORT buffers
      gpus: all
      restart: unless-stopped
      volumes:
        - .:/app
      command: sleep infinity
  
  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama   # cache models here
    gpus: all

volumes:
  ollama-models: